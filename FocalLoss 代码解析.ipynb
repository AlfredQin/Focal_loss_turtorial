{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focalloss 代碼解析\n",
    "\n",
    "\n",
    "### CrossEntrypy\n",
    "\n",
    "- $CE(P_t) = -log(pt)$\n",
    "\n",
    "$pt = \\frac{e^{x[class]}}{\\sum_j e^{x[j]}}$ 　　經過softmax運算\n",
    "\n",
    "\n",
    "\n",
    "$Loss(x, class) = -log(\\frac{e^{x[class]}}{\\sum_j e^{x[j]}}\\quad)^{\\, \\, \\gamma} $\n",
    "\n",
    "\n",
    "### Focal Loss\n",
    "\n",
    "\n",
    "- $FL(P_t) = -\\alpha(1-p_t)^rlog(pt)$\n",
    "\n",
    "$Loss(x, class) = -\\alpha_class ( 1- \\frac{e^{x[class]}}{\\sum_j e^{x[j]}}\\,\\,\\,\\,    )^{\\gamma}log(\\frac{e^{x[class]}}{\\sum_j e^{x[j]}}\\,\\,\\,\\,)$\n",
    "\n",
    "　　　　　　　= $-\\alpha_{class} ( 1- softmax(x)[class])^\\gamma * log(softmax(x)[class])$\n",
    "       \n",
    "\n",
    "       \n",
    "$\n",
    "\\alpha_t =\\begin{cases}\n",
    "\\alpha ,  \\quad if\\quad  y =１\\\\\n",
    "1 - \\alpha, \\quad otherwise\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "- $\\gamma$ 负责降低简单样本的损失值, 以解决加总后负样本loss\n",
    "- $\\alpha$ 调和正负样本的不平均，如果设置0.25, 那么就表示负样本为0.75, 对应公式$1 - \\alpha$\n",
    "\n",
    "### 从公式可以看出\n",
    "\n",
    "控制样本权重的为 $\\alpha(1-p_t)^\\gamma$\n",
    "\n",
    "当$p_t$越大，赋予的权重就越小， $p_t$越小，赋予的权重就越大\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### 解决问题\n",
    "\n",
    "基于原来多分类損失函數CrossEntropy进行改进，最初one-stage目标检测框架有easy-example（背景） 和 hard-example（前景）严重样本分布不均的问题，往往easy-example的loss与hard-example的存在极大的不平衡(1：1000)，导致模型都在学习easy-example而忽略了hard-example\n",
    "\n",
    "<img src=\"https://github.com/Stephenfang51/Focal_loss_turtorial/blob/master/images/focal_gamma.png?raw=true\" width=800>\n",
    "\n",
    "\n",
    "\n",
    "根据图表，基于CE的公式， 提出了新的因子 $-(1-p_t)^\\gamma$,  当 $\\gamma$ 值>0, 减低了easy_example(pt>0.5)的loss值，因此模型能够更专注在学习hard_example\n",
    "\n",
    "PS.  作者发现$\\gamma$ 的初始值2为最佳\n",
    "\n",
    "\n",
    "### 举个简单的例子帮助我们快速理解\n",
    "\n",
    "\n",
    "\n",
    "假设我们模型分类负样本10000笔资料，probability(pt) = 0.95， 这边可以理解为easy-example因为概率高\n",
    "\n",
    "正样本10笔资料， probability(pt) = 0.05， 可以理解为hard-example 概率低\n",
    "\n",
    "直接带入CE和FL\n",
    "\n",
    "1. 带入CrossEntropy - $CE(P_t) = -log(pt)$\n",
    "\n",
    "    - 负样本 ： log(p_t) * 样本数（100000） = 0.02227 * 100000 = 2227\n",
    "    - 正样本 ： log(p_t) * 样本数（10） = 1.30102 * 10 = 13.0102\n",
    "    total loss = 2227+13.0102 = 2240\n",
    "    正样本占比：13.0102 / 2240 = 0.0058\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "2. 带入Focalloss\n",
    "\n",
    "假设alpha = 0.25（正样本， gamma=2\n",
    "\n",
    "\n",
    "    - 负样本 ： 0.75*（1-0.95)^2 * 0.02227 *样本数（100000） = 0.00004176 * 100000 = 4.1756\n",
    "    - 正样本 ： 0.25* (1-0.05)^2 * 1.30102 *样本数（10）= 0.29354264 * 10 = 2.935\n",
    "    total loss = 4.175 + 2.935 = 7.110\n",
    "    正样本占比：2.935/7.110 = 0.4127（与0.0058差距甚大）\n",
    "\n",
    "\n",
    "\n",
    "### 小结：\n",
    "\n",
    "1. gamma = 2时候， 负样本 $(1-0.95)^2$  = 0.0025， 正样本$(1-0.05)^2$ = 0.9025, 负样本损失值明显比正样本小很多\n",
    "2. alpha与gamma是一种相互平衡的值，虽然就理论上来看，alpha值设定为0.75(因为正样本通常数量小)是比较合理， 但是配合gamma值已经将负样本损失值降低许多，可理解为alpha和gamma相互牵制，alpha也不让正样本占比太大，因此最终设定为0.25\n",
    "\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "论文连接 https://arxiv.org/abs/1708.02002\n",
    "\n",
    "pytorch源碼实践 https://github.com/marvis/pytorch-yolo2/blob/master/FocalLoss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L = -\\sum^C_i onehot \\bigotimes \\alpha(1 - P_i)^\\gamma log P_i$\n",
    "\n",
    "$\\bigotimes$ = element-wise 乘法运算\n",
    "\n",
    "\n",
    "整个代码执行思路围绕上述公式，简单简洁\n",
    "\n",
    "### 代码执行思路\n",
    "\n",
    "   1. 首先取得$(P_i)$：对input进行softmax，取得概率(probability)\n",
    "   2. 基于input的shape制作class_mask(one_hot) 元素全为0\n",
    "   3. 将targets(label)的索引值丢回class_mask，就生成了one_hot形式的targets（目标值为1, 其余全为0）ex.[0, 0, 0, 1, 0]\n",
    "   4. $[(P_i) \\bigotimes classmask(onehot)]$ 并且求log对数 = $log_{pi}$ $\\bigotimes$ 表示element-wise\n",
    "   5. 带回公式， 此时$log_{P_i}$ 已经得到， alpha值和gamma值作为超参数传入\n",
    "       - $L = -\\sum^C_i onehot \\bigotimes \\alpha(1 - P_i)^\\gamma log P_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        if alpha is None:\n",
    "            self.alpha = t.ones(class_num, 1)\n",
    "        else:\n",
    "            if alpha:\n",
    "#                 self.alpha = t.ones(class_num, 1, requires_grad=True)\n",
    "                self.alpha = t.tensor(alpha, requires_grad=True)\n",
    "                print('alpha初始\\n', self.alpha)\n",
    "                print('alpha shape\\n', self.alpha.shape)\n",
    "#             else:\n",
    "#                 self.alpha = t.ones(class_num, 1*alpha).cuda()\n",
    "        self.gamma = gamma\n",
    "        self.class_num = class_num\n",
    "        self.size_average = size_average\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        #input.shape = (N, C)\n",
    "        N = inputs.size(0)\n",
    "        C = inputs.size(1)\n",
    "        P = F.softmax(inputs) #經過softmax 概率\n",
    "        #---------one hot start--------------#\n",
    "        class_mask = inputs.data.new(N, C).fill_(0)  #生成和input一样shape的tensor\n",
    "        print('依照input shape制作:class_mask\\n', class_mask)\n",
    "        class_mask = class_mask.requires_grad_() #需要更新， 所以加入梯度计算\n",
    "        ids = targets.view(-1, 1) #取得目标的索引\n",
    "        print('取得targets的索引\\n', ids)\n",
    "        class_mask.data.scatter_(1, ids.data, 1.) #利用scatter将索引丢给mask\n",
    "        print('targets的one_hot形式\\n', class_mask) #one-hot target生成\n",
    "        #---------one hot end-------------------#\n",
    "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
    "            self.alpha = self.alpha.cuda()\n",
    "#         alpha = self.alpha[ids.data.view(-1, 1)]\n",
    "#         alpha = self.alpha[ids.view(-1)]\n",
    "        alpha = self.alpha\n",
    "        print('alpha值\\n', alpha)\n",
    "        print('alpha shape\\n', alpha.shape)\n",
    "        \n",
    "        probs = (P*class_mask).sum(1).view(-1, 1) \n",
    "        print('留下targets的概率（1的部分），0的部分消除\\n', probs)\n",
    "        #将softmax * one_hot 格式，0的部分被消除 留下1的概率， shape = (5, 1), 5就是每个target的概率\n",
    "        \n",
    "        log_p = probs.log()\n",
    "        print('取得对数\\n', log_p)\n",
    "        #取得对数\n",
    "        \n",
    "        batch_loss = -alpha*(t.pow((1-probs), self.gamma))*log_p #對應下面公式\n",
    "        print('每一个batch的loss\\n', batch_loss)\n",
    "        #batch_loss就是取每一个batch的loss值\n",
    "        \n",
    "        \n",
    "        #最终将每一个batch的loss加总后平均\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "        print('loss值为\\n', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带入模拟数值看结果 - Focalloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input值为\n",
      " tensor([[-1.1588,  0.3673,  0.7110, -0.2373, -1.0129],\n",
      "        [ 0.5580, -0.8784, -1.1446, -0.7629, -0.0170],\n",
      "        [-0.0477,  0.1770, -0.6058,  0.0125,  0.6818],\n",
      "        [ 0.4508,  0.4299,  1.0491,  0.0453, -1.5092],\n",
      "        [-1.5502, -0.7564,  0.6274, -0.4808, -0.6856]], requires_grad=True)\n",
      "targets值为\n",
      " tensor([1, 0, 4, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "t.manual_seed(50) #随机种子确保每次input tensor值是一样的\n",
    "input = torch.randn(5, 5, dtype=torch.float32, requires_grad=True)\n",
    "print('input值为\\n', input)\n",
    "targets = t.randint(5, (5, ))\n",
    "print('targets值为\\n', targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha初始\n",
      " tensor(0.2500, requires_grad=True)\n",
      "alpha shape\n",
      " torch.Size([])\n",
      "依照input shape制作:class_mask\n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "取得targets的索引\n",
      " tensor([[1],\n",
      "        [0],\n",
      "        [4],\n",
      "        [3],\n",
      "        [4]])\n",
      "targets的one_hot形式\n",
      " tensor([[0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]], requires_grad=True)\n",
      "alpha值\n",
      " tensor(0.2500, requires_grad=True)\n",
      "alpha shape\n",
      " torch.Size([])\n",
      "留下targets的概率（1的部分），0的部分消除\n",
      " tensor([[0.2920],\n",
      "        [0.4445],\n",
      "        [0.3480],\n",
      "        [0.1447],\n",
      "        [0.1370]], grad_fn=<ViewBackward>)\n",
      "取得对数\n",
      " tensor([[-1.2312],\n",
      "        [-0.8108],\n",
      "        [-1.0556],\n",
      "        [-1.9329],\n",
      "        [-1.9875]], grad_fn=<LogBackward>)\n",
      "每一个batch的loss\n",
      " tensor([[0.1543],\n",
      "        [0.0625],\n",
      "        [0.1122],\n",
      "        [0.3535],\n",
      "        [0.3700]], grad_fn=<MulBackward0>)\n",
      "loss值为\n",
      " tensor(0.2105, grad_fn=<MeanBackward1>)\n",
      "tensor([[ 0.0192, -0.2146,  0.1248,  0.0483,  0.0223],\n",
      "        [-0.1181,  0.0225,  0.0172,  0.0252,  0.0532],\n",
      "        [ 0.0455,  0.0570,  0.0260,  0.0483, -0.1769],\n",
      "        [ 0.0788,  0.0772,  0.1434, -0.3105,  0.0111],\n",
      "        [ 0.0210,  0.0465,  0.1856,  0.0613, -0.3145]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "criterion = FocalLoss(5, alpha=0.25, gamma=2, size_average=True)\n",
    "loss = criterion(input, targets)\n",
    "loss.backward()\n",
    "# print(input.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带入Cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4036)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(input, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  以上能明显感觉到一开始的loss值已经有很大差距"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
